{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from skimage import filters\n",
    "import sys\n",
    "sys.path.append('/Users/kategroschner/Documents/pyNanoFind/utilities/')\n",
    "import realDataProcess as rdp\n",
    "from skimage import exposure\n",
    "from skimage import morphology as morph\n",
    "from skimage.util import invert\n",
    "from skimage import feature\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = h5py.File('/Users/kategroschner/Downloads/Chiral_D_2018_10_31_FilteredStack.emd','r')\n",
    "subdata = data['data']\n",
    "subdata = subdata['Masked Particles']\n",
    "subdata = subdata['data'][:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.loadtxt('/Users/kategroschner/Downloads/Chiral_D_ManualValues_2018_10_31.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_noFive(images, labels, min_num_labels):\n",
    "    final_image = []\n",
    "    final_labels = []\n",
    "    count_left = 0\n",
    "    count_right = 0\n",
    "    count_garbage = 0\n",
    "    #count_idk = 0\n",
    "    for idx, l in enumerate(labels):\n",
    "        if l == 4:\n",
    "            if count_left < min_num_labels:\n",
    "                final_image.append(images[idx])\n",
    "                final_labels.append(l)\n",
    "                count_left+=1\n",
    "        if l == 5:\n",
    "            if count_garbage < min_num_labels:\n",
    "                final_image.append(images[idx])\n",
    "                final_labels.append(0)\n",
    "                count_garbage += 1\n",
    "        if l == 6:\n",
    "            if count_right < min_num_labels:\n",
    "                final_image.append(images[idx])\n",
    "                final_labels.append(l)\n",
    "                count_right += 1\n",
    "        if l == 0:\n",
    "            if count_garbage < min_num_labels:\n",
    "                final_image.append(images[idx])\n",
    "                final_labels.append(l)\n",
    "                count_garbage += 1\n",
    "    return np.array(final_image),np.array(final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_images, sort_labels = balance_noFive(subdata,labels,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(img_stack,labels):\n",
    "    vals = np.arange(0,len(labels))\n",
    "    random.shuffle(vals)\n",
    "    new_stack = np.zeros(img_stack.shape)\n",
    "    new_labels = np.ones(labels.shape)\n",
    "    for old_idx,new_idx in enumerate(vals):\n",
    "        new_stack[old_idx] = img_stack[new_idx]\n",
    "        new_labels[old_idx] = labels[new_idx]\n",
    "    return new_stack,new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_images, final_labels = shuffle(sort_images,sort_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('particles_nofive.npy',final_images)\n",
    "np.save('labels_nofive.npy',final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = final_images[:,50:150,50:150]\n",
    "images = np.expand_dims(images,axis = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls = final_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls[lbls == 4] = 1\n",
    "lbls[lbls == 6] =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(100, 100,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = images[:175]\n",
    "trainY = labels[:175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "valX = images[175:225]\n",
    "valY = labels[175:225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(trainX, y=trainY, batch_size=batch_size,seed=seed)\n",
    "val_generator = test_datagen.flow(valX,y=valY,batch_size=batch_size,seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 98, 98, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 98, 98, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 47, 47, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 47, 47, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 21, 21, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 21, 21, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                409664    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 437,923\n",
      "Trainable params: 437,923\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31/31 [==============================] - 6s 191ms/step - loss: 1.0120 - acc: 0.4709 - val_loss: 0.7821 - val_acc: 0.6200\n",
      "Epoch 2/10\n",
      "31/31 [==============================] - 7s 225ms/step - loss: 0.8687 - acc: 0.5511 - val_loss: 0.7228 - val_acc: 0.6800\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 0.8750 - acc: 0.5667 - val_loss: 0.6695 - val_acc: 0.7600\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - 6s 184ms/step - loss: 0.8517 - acc: 0.6082 - val_loss: 0.6261 - val_acc: 0.7400\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - 5s 177ms/step - loss: 0.8642 - acc: 0.5984 - val_loss: 0.7135 - val_acc: 0.6600\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - 6s 191ms/step - loss: 0.8465 - acc: 0.6167 - val_loss: 0.6714 - val_acc: 0.6600\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - 6s 180ms/step - loss: 0.7741 - acc: 0.6690 - val_loss: 0.5967 - val_acc: 0.7600\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - 6s 183ms/step - loss: 0.8039 - acc: 0.6643 - val_loss: 0.6062 - val_acc: 0.7800\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - 5s 176ms/step - loss: 0.7492 - acc: 0.6823 - val_loss: 0.5595 - val_acc: 0.8000\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - 6s 182ms/step - loss: 0.6903 - acc: 0.7032 - val_loss: 0.5233 - val_acc: 0.8400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13995a198>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=500 // batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=200 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(images[225:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   1.00000000e+00],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00],\n",
       "       [  1.00000000e+00,   1.37820780e-18,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   2.16236117e-25,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  4.43175980e-29,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00],\n",
       "       [  4.00618857e-30,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  6.88517474e-08,   9.99999881e-01,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  1.12525585e-32,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bin = predicted.copy()\n",
    "pred_bin[pred_bin<0.95] = 0\n",
    "pred_bin[pred_bin>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_lbl = []\n",
    "for i in pred_bin:\n",
    "    if i[0] == 1:\n",
    "        p_lbl.append(0)\n",
    "    elif i[1] == 1:\n",
    "        p_lbl.append(1)\n",
    "    elif i[2] == 1:\n",
    "        p_lbl.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_lbl = np.array(p_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_lbl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56000000000000005"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(lbls[225:250],p_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   1.00000000e+00],\n",
       "       [  2.27130633e-02,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   8.18424800e-04],\n",
       "       [  1.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   1.65321794e-03,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  9.99733031e-01,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00],\n",
       "       [  1.00000000e+00,   1.61730333e-27,   0.00000000e+00],\n",
       "       [  2.65318931e-07,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  5.63265536e-29,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  1.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   7.03940649e-13,   1.98814634e-37],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('basic_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 100, 100\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "epochs = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import gray2rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_rbg = np.zeros((175,100,100,3))\n",
    "for idx,img in enumerate(trainX):\n",
    "    t = img[:,:,0]\n",
    "    trainX_rbg[idx] = gray2rgb(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175, 100, 100, 1)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valX_rbg = np.zeros((50,100,100,3))\n",
    "for idx,img in enumerate(valX):\n",
    "    t = img[:,:,0]\n",
    "    valX_rbg[idx] = gray2rgb(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet', input_shape=(100,100,3))\n",
    "\n",
    "    generator = datagen.flow(\n",
    "        trainX_rbg,\n",
    "        y = trainY,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, len(trainX_rbg) // batch_size)\n",
    "    np.save('bottleneck_features_train.npy',\n",
    "            bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow(\n",
    "        valX_rbg,\n",
    "        y = valY,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, len(valX_rbg) // batch_size)\n",
    "    np.save('bottleneck_features_validation.npy',\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "\n",
    "def train_top_model():\n",
    "    train_data = np.load('bottleneck_features_train.npy')\n",
    "    train_labels = np.array(\n",
    "        [0] * (len(trainX_rbg) // 2) + [1] * (len(trainX_rbg) // 2))\n",
    "\n",
    "    validation_data = np.load('bottleneck_features_validation.npy')\n",
    "    validation_labels = np.array(\n",
    "        [0] * (len(valX_rbg) // 2) + [1] * (len(valX_rbg) // 2))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x93 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-633e7c6f69b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msave_bottlebeck_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_top_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-97-1f67f59d54a1>\u001b[0m in \u001b[0;36mtrain_top_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_top_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bottleneck_features_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     train_labels = np.array(\n\u001b[1;32m     31\u001b[0m         [0] * (nb_train_samples / 2) + [1] * (nb_train_samples / 2))\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0m_ZIP_PREFIX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'PK\\x03\\x04'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;31m# If the file size is less than N, we need to make sure not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;31m# to seek past the beginning of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x93 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 160 input samples and 174 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-0da2181ac8b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_top_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-106-c44b7971afe0>\u001b[0m in \u001b[0;36mtrain_top_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m               validation_data=(validation_data, validation_labels))\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_model_weights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m             \u001b[0m_check_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1491\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[1;32m   1492\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_check_array_lengths\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    218\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 160 input samples and 174 target samples."
     ]
    }
   ],
   "source": [
    "train_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
